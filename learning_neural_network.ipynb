{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 주도 학습\n",
    "- 학습의 방법들\n",
    "    - 사람이 생각한 알고리즘\n",
    "    - 사람이 생각한 특징 -> 기계학습\n",
    "    - 신경망(딥러닝)\n",
    "        - 위의 두개는 사람의 개입이 있지만 딥러닝은 중요한 특징까지 기계가 스스로 학습\n",
    "        - 종단간 기계학습(end-to-end machine learning)이라고도 함\n",
    "- 학습 시 주의할 점\n",
    "    - overfitting\n",
    "\n",
    "## 손실 함수\n",
    "- 신경망 학습은 특정 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것\n",
    "    - 얼마나 잘 학습 됐는지를 보여주는 지표가 손실함수\n",
    "    - 손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표\n",
    "        - 즉 클수록 나쁜 것을 의미함   \n",
    "    - 주로 사용되는 손실 함수는 오차제곱합(sum of squares for error, SSE), 교차 엔트로피 오차(cross entropy error, CEE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#오차제곱합 구현\n",
    "#MNIST 데이터 셋이라고 가정하고 실습 함\n",
    "#아래는 0~9까지의 숫자일 확률을 softmax함수로 출력했다고 생각함\n",
    "y1 = [0.1,0.05,0.6,0,0.05,0.1,0,0.1,0,0]\n",
    "y2 = [0.1,0.05,0.1,0,0.05,0.1,0,0.6,0,0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "print(sum_squares_error(np.array(y1), np.array(t)))\n",
    "print(sum_squares_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "#교차 엔트로피 오차\n",
    "#이 값은 정답일 때의 출력이 전체 값을 정하게 됨\n",
    "#오차제곱합은 모든 출력이 값을 결정하는데 이와는 다름\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    #0이 입력됐을 때 -inf로 만들지 않기 위한 조금의 보정\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "\n",
    "y1 = [0.1,0.05,0.6,0,0.05,0.1,0,0.1,0,0]\n",
    "y2 = [0.1,0.05,0.1,0,0.05,0.1,0,0.6,0,0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "print(cross_entropy_error(np.array(y1), np.array(t)))\n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "#모든 데이터를 다 평가하면 너무 크기 때문에 10개의 값만 배치로 가져옴\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t* np.log( y+ 1e-7 )) / batch_size\n",
    "\n",
    "print(cross_entropy_error(np.array(y1), np.array(t)))\n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수를 사용하는 이유\n",
    "- 어짜피 정확도를 구하는데 왜 손실 함수의 값이라는 우회적인 방법을 택하는가?\n",
    "    - 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾음\n",
    "    - 가중치 매개변수의 값을 아주 조금 변화시킬 때 손실 함수가 어떻게 변하는지를 가지고 배개변수 값을 변화시킴\n",
    "    - 정확도를 지표로 삼으면 미분값이 대부분의 장소에서 0이 되기 때문에 손실 함수를 사용함\n",
    "        - 정확도를 지표로 삼으면 불연속 적으로 30 % -> 32% -> 38% 처럼 불연속 적으로 나타남\n",
    "        - 계단 함수를 생각해보면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp7UlEQVR4nO3dd3xV9f3H8dc3A8IIe48Q9h5CBuJG3ANt1QqoIKvaun5tbW2ttbXD1ta2WrXKciDgnog4AEW0BBI2hLAhAUISCBmQne/vj3PRGBJIIOee5Ob9fDx4kOSce78fzr28OXw/53uusdYiIiKBJ8jrAkRExB0KeBGRAKWAFxEJUAp4EZEApYAXEQlQIV4XUFabNm1sZGSk12WIiNQZCQkJGdbathVtq1UBHxkZSXx8vNdliIjUGcaYvZVt0xSNiEiAUsCLiAQoBbyISIByNeCNMS2MMW8ZY7YaYxKNMee6OZ6IiHzH7SbrU8Bia+1NxpgGQGOXxxMRER/XAt4Y0wy4EJgEYK0tBArdGk9ERL7PzSmaHkA68KIxZq0xZpYxpomL44mISBluBnwIMBz4r7X2HOAY8FD5nYwx040x8caY+PT0dBfLERGpfRL2ZjJz+S5XntvNgE8BUqy1cb7v38IJ/O+x1s6w1kZZa6Patq1wMZaISEDamJLFpDmrmBe3l9yC4hp/ftcC3lqbCiQbY/r6fnQpsMWt8URE6pItB7K5fU4czRqFMm/aSJo2rPmWqNtX0dwLzPNdQbMLuNPl8UREar2k1Bxumx1Ho9BgFkwbSecWjVwZx9WAt9auA6LcHENEpC7ZkZbDhFkrCQkyzJ82kojW7l09rpWsIiJ+sjM9l3Ez4wDDgukj6d7G3QsLFfAiIn6wJ+MY42eupLTUsmBaLD3bNnV9TAW8iIjLko8cZ/zMlRQWlzJ/2kh6tw/3y7i16n7wIiKBJiXzOLfOWMmxwhLmT4ulbwf/hDvoDF5ExDUHs/IYPzOO7PwiXp0Sy8BOzf06vgJeRMQFh7LzGTdjJZnHCpk7JZbBXfwb7qCAFxGpcWk5+YybuZL0nAJemhzDsK4tPKlDc/AiIjUoI7eACTPjSM3K5+XJMYzo1tKzWnQGLyJSQ06Ee3LmceZMiiY6spWn9egMXkSkBqTnFDB+5kqSM48ze2I0I3u09rokBbyIyNlKy8ln/Mw49mfmMWdSNKN6tvG6JEABLyJyVtKynYbqgaP5vHhn7ThzP0EBLyJyhk5cCpma7TRUY7p7O+dengJeROQMpGY5Z+5p2fm8MjmGKI8bqhVRwIuIVNPBrDzGzVhJRm4hr0yJYUS32hfuoIAXEamW/Ufzvl2h+sqUGIZHeHed++ko4EVEqigl8zjjZq7k6PEi5k6N9WyFalUp4EVEqiD5iBPu2XnOjcOG1vJwBwW8iMhpJR9xbvmbW1DMvKkjPblx2JlQwIuInMKu9FzGz4wjr6iEeVNjGdS5boQ7KOBFRCqVlJrDhFlxWGt5bfpI+nds5nVJ1aKAFxGpwKb9Wdw+O44GIUHMm3ouvdq5/xmqNU0BLyJSTsLeTCa9uIpmYaHMnxZLt9ZNvC7pjLga8MaYPUAOUAIUW2uj3BxPRORs/W/nYaa8vJp24Q2ZN20knVs08rqkM+aPM/hLrLUZfhhHROSsfLktnemvxBPRqjHzpsbSrlmY1yWdFU3RiIgAn25O5Z75a+nVrilzp8TQumlDr0s6a25/opMFPjXGJBhjprs8lojIGflw/QHunreG/p2asWDayIAId3D/DP48a+0BY0w74DNjzFZr7fKyO/iCfzpARESEy+WIiHzfm/HJ/OrtDUR1a8XsSVGEh4V6XVKNcfUM3lp7wPd7GvAuEFPBPjOstVHW2qi2bdu6WY6IyPfMXbmXB9/awHm92vDy5JiACndwMeCNMU2MMeEnvgYuBza5NZ6ISHU8/+VOHnlvE2P6t2PmHVE0ahDsdUk1zs0pmvbAu8aYE+PMt9YudnE8EZHTstbyxCdJ/PeLnVw7pCP/vGUYDULcbkd6w7WAt9buAoa69fwiItVVUmp55P1NzI/bx4TYCB4bO4jgION1Wa7RZZIiUi8UFpfyszfWsXDDQX5ycU8evKIvvhmGgKWAF5GAl1dYwt3zEvgiKZ2HrurHXRf19Lokv1DAi0hAy8orYurLq4nfm8njPxjMuJj6czm2Al5EAlZ6TgET56xie1oOz4wbzjVDOnpdkl8p4EUkIO0/msdts+I4mJXHrInRXNSn/q2zUcCLSMDZkZbL7bPjyC0o5tUpsURFtvK6JE8o4EUkoGzan8Udc1YRZAyvTz+XAZ3q1qcw1SQFvIgEjG92ZDB9bgLNG4Xy6tRYurepmx/UUVMCc/mWiNQ7CzccYOKLq+jcohFv3X1uvQ930Bm8iASAl77ezR8WbiG6Wytm3hFF88aBddOwM6WAF5E6y1rL3z9J4rkvdnL5gPY8Pe4cwkID76ZhZ0oBLyJ1UnFJKb9+ZyNvJqQwPjaCPwb4fWXOhAJeROqcvMISfjp/DUu3pvHAmN7cf2nvgL+vzJlQwItInZJ5rJDJL69mffJR/nTDIG4b2c3rkmotBbyI1Bn7j+Zxx+w4kjPzeG7CCK4c1MHrkmo1BbyI1AlbU7OZOGcVxwtLmDs5htgerb0uqdZTwItIrbdq9xGmvLyaxg2CefOuc+nXof6uTq0OBbyI1Gofrj/Az99YT5dWjXhlcgxdWjb2uqQ6QwEvIrWStZbnv9zF3xZvJSayFTPuGEGLxg28LqtOUcCLSK1TXFLKox9sZl7cPq4b2om/3zREC5jOgAJeRGqVYwXF3LtgLUu3pnHXRT355RV9CdICpjOigBeRWiMtJ5/JL61my4FsXeNeAxTwIlIrbD+Uw6QXV3PkWCEz74ji0v7tvS6pznM94I0xwUA8sN9ae63b44lI3bNy12GmvxJPg5BgXv/xSIZ0aeF1SQHBH/eDvx9I9MM4IlIHvb9uP7fPjqNdszDe/ckohXsNcjXgjTFdgGuAWW6OIyJ1j7WWZ5ft4P7X1jE8oiVv3zWKrq10jXtNcnuK5t/AL4HwynYwxkwHpgNERES4XI6I1AaFxaX89r2NvBGfwthhnXjipiE0DNFlkDXNtTN4Y8y1QJq1NuFU+1lrZ1hro6y1UW3btnWrHBGpJTKPFXL77DjeiE/h3tG9+NctwxTuLnHzDP484HpjzNVAGNDMGPOqtfY2F8cUkVpsZ3ouU15azYGj+fz7R8O44ZzOXpcU0Fw7g7fW/tpa28VaGwncCixVuIvUX9/syODGZ78mJ7+Y+dNiFe5+oOvgRcR1C1bt45H3NtG9TRPmTIpWM9VP/BLw1tovgC/8MZaI1B4lpZbHFyUya8VuLuzTlmfGn0OzsFCvy6o3dAYvIq44VlDM/a+t5fPENCae241Hrh1ASLA/lt7ICQp4EalxB47mMeXleJJSs/nD9QOZOCrS65LqJQW8iNSodclHmfZKPPmFJcyZFM3Ffdt5XVLtdTQZEl6Ewzvhlpdr/OkV8CJSY95ft59fvrWBtuENmTc1lj7tK13jWH+VlsLuL2DVLNj2MVgLfa+C4gIIaVijQyngReSslZRanvhkKy98uYuYyFY8d9tw2jSt2bCq8/KOwvoFsHoWHN4BjVvDeffDiDuhpTu3RVbAi8hZycor4v7X1vJFUjrjYyP4/XUDaRCiZuq3UjfCqpmw8U0oOg5douHGGTBgLISGuTq0Al5EztjO9FymvRLPvsPH9QEdZRUXwpb3nbP15JUQEgaDb4LoadBpmN/KUMCLyBlZlpTGfQvWEhocxKtTYxnZo7XXJXkvKwXiX4Q1L8OxdGjZHS7/MwwbD41b+b0cBbyIVIu1lhnLd/HXxVvp16EZM24fUb9XploLu75wztaTFjnf97kSoqdCz9EQ5N10lQJeRKosv6iEh97ewHvrDnDN4I78/eYhNG5QT2Pk26bpbDi8HRq1glH3QdRk15qm1VVPXxkRqa6DWXn8eG4CG1Ky+MXlffjpJb0wxnhdlv+lboLVM2HDG07TtHMU3PgCDLjB9aZpdSngReS0EvZm8uO5CeQVFjPzjiguG1DPPhC7uBASP3Cuhvle03QqdDrH6+oqpYAXkUpZa3k1bh+PfbiZTi0aMX9aPVu8VGHT9E8wbIInTdPqUsCLSIXyi0p4+N1NvL0mhUv6tuXfPzqH5o3rwZ0gK2yaXuFc4uhx07S6FPAicpLkI8e569UENh/I5v5Le3P/pb0JCgrw+fb8LFh3YqXpiabpvb6maaTX1Z0RBbyIfM+X29K5b8FarLXMmRTF6H4BPt9+UtN0BNzwPAy8sdY1TatLAS8iAJSWWp77YgdPfraNvu3DeeH2EXRr3cTrstxxomm6ehbs+5/TNB10E0RPgc7Dva6uxijgRYTs/CJ+9vp6Pk88xNhhnXj8B4MD8/r2rP3O7XkTXoZjac7Uy2V/hHNuqxNN0+oKwFdQRKojKTWHu15NIPnIcR69bgCTRkUG1vXt1sLuL52z9a2LwJZC78shZhr0vLRONU2rSwEvUo99uP4Av3xrA03DQlgwfSTRkQF0FpufBetfc4I9Y5uvaXqPc3veVt29rs4vFPAi9VBhcSmPf5zIi1/vYUS3ljw3YTjtm9XthuK3Dm12FiRteAOKjkGn4XDDf31N00ZeV+dXCniReiYl8zg/nb+W9clHmTQqkt9c3b/u37/926bpbNj3DQQ39K00neJcFVNPKeBF6pEliYf42RvrnStmJgzn6sEdvS7p7GTth4SXnJWmuYegRTe47DE45/aAbJpWl2sBb4wJA5YDDX3jvGWtfdSt8USkckUlpfzj0yRe+HIXAzo247kJw4lsU0cvgbQWdi93rl0v2zSNngq9xgR007S63DyDLwBGW2tzjTGhwApjzMfW2pUujiki5aRm5XPvgjWs3pPJ+NgIfnftAMJCg70uq/rys8s0TZOgUUs496fOStN60jStLtcC3lprgVzft6G+X9at8UTkZMu3pfPA6+vILyrhqVuHMXZYZ69Lqr5DW5yz9fWv1/umaXVVKeCNMe2A84BOQB6wCYi31pae5nHBQALQC3jWWhtXwT7TgekAERER1SpeRCpWUmr59+fbeGbZDnq3a8pzE0bQq11Tr8uquuJC2Pqh0zTd+7XTNB30Q4iZWq+bptVlnBPtSjYacwnwENAKWAukAWFAH6An8BbwpLU2+5SDGNMCeBe411q7qbL9oqKibHx8fDX/CCJSVlpOPvcvWMf/dh3m5hFdeGzsIBo1qCNTMtkHnKZpwkvfNU2jp6hpegrGmARrbVRF2053Bn81MM1au6+CJw0BrgUuA94+1ZNYa48aY74ArsQ5+xcRF6zYnsEDr68jt6CIJ24awi1RXb0u6fSshT1fOdeub/3I1zS9rEzTtI7841QLnTLgrbUPnmJbMfBeZduNMW2BIl+4NwLGAH87wzpF5BSKSkp58tNtvLB8Jz3aNOHVqTH069DM67JOrcKm6U98TdMeXlcXEKo6Bz8XuMdam+X7PhKYba299BQP6wi87JuHDwLesNYuPMt6RaScfYePc+9rzsKlcTFdeeTaAbX7RmGHtjihvuF1KMx1PvJu7HMw6Adqmtawqr4LVgBxxpifAZ2BB4Gfn+oB1toNQO39sEKRAPD+uv08/O4mjIFnxw/nmiG1dOFSSREkfugEe9mmafRU6KKmqVuqFPDW2heMMZuBZUAGcI61NtXVykSkUscKinn0g828lZDCiG4teerWYXRp2djrsk6WfcC5NW/CS5CbCi0iYMwfnKZpk9ZeVxfwqjpFczvwCHAHMARYZIy501q73s3iRORkm/Zncd+Ctew+fIx7R/fi/kt7ExJci1ZvWgt7VjjXricudJqmvcZAzNNqmvpZVadofgicb61NAxYYY94FXkJTMCJ+Y61lztd7+NvHW2nZJJT5U0dybs9adBacn+3Mq6+eBelbIawFjLzbucxRTVNPVHWK5oZy368yxsS6UpGInORwbgG/eHM9y5LSGdO/PU/cNIRWTRp4XZYjLdEJ9fWvOU3TjsNg7LPOHLuapp46ZcAbY34LPGetPVJ+m7W20BgzGmisq2NE3LMsKY1fvrWBrLwiHhs7kNtHdvP+E5dKimDrQlg1C/au8DVNfwDR05zPNPW6PgFOfwa/EfjQGJMPrAHScVay9gaGAZ8Df3GzQJH6Kq+whL8sSmTuyr30bR/OK5Nj6N/R42vbsw+WWWmaCs0jYMzv4Zw71DSthU4X8DdZa88zxvwS5zYFHYFs4FVgurU2z+0CReqj9clH+b/X17Er4xhTz+/OL67o690dIE9qmpY4zdLop5wVp2qa1lqnC/gRxphuwATgknLbGuHceExEakhxSSnPfbGTp5dsp214Q+ZPjWVUrzbeFFOQ41tpOhvSE79rmkZNhtY9valJquV0Af88sBjoAZS9C5jBufWvWuMiNWTv4WM88Po61u47ythhnXjs+kE0bxzq/0JOapoOheufcZqmDWrhtfZSqdPdi+Zp4GljzH+ttXf7qSaResVay2urk/njwi2EBBmeHncO1w/t5N8iTjRNV892bvwV3AAG/gBipjm351XTtE6q6mWSCncRF2TkFvDQ2xv5PPEQo3q25h83D6VTCz9eWpiT+l3TNOeg0zS99FEYfgc08WhqSGpMLb4jkUhg+2zLIX79zgay84t55NoB3DkqkqAgP5wpW+vcD2bVTOesvbQYel4K1/7L+WxTNU0DhgJexM+yjhfxhw83887a/fTv2Ix5U4fRt0O4+wOf1DRtDrF3qWkawBTwIn60dOshfv3ORjJyC7nv0t7cc0kvGoS4fB+ZtK1lmqY50GEIXP8fGHSTmqYBTgEv4gdZeUX8aeEW3kxIoW/7cGZPjGZQ5+buDVhS5Hw60upZZZqmNzorTbtEqWlaTyjgRVz25bZ0Hnp7A4ey8/npJT2579LeNAxxaZ47J9V3e94XfU3Trk7T9JzboWlbd8aUWksBL+KSnPwi/vxRIq+tTqZ3u6Y8/5PzGNq1Rc0PZC3s/ca30vTD75qm1/wT+lyhpmk9poAXccFX29P51VsbSM3O566LevLAmN41f6uBghzf7XlnQ9oWp2ka82Pn9rxqmgoKeJEalXW8iD995My192jbhLfuHsXwiJY1O0h6kjO3vm6BmqZySgp4kRry8caDPPL+ZjKPF/KTi5259ho7ay8phqSPnGvXv9c0nQpdotU0lQop4EXOUlp2Po+8v4lPNh9iYKdmvHRnDV4h823T9CXIOeBrmv7OuT2vmqZyGgp4kTNkreWN+GT+9FEihcWl/OrKfky7oPvZfz6qtbDvf87ZeuIHvqbpaLjmSTVNpVoU8CJnYO/hY/z6nY18s/Mwsd1b8dcfDqF7myZn96QFuWWappu/a5pGTYY2vWqmcKlXXAt4Y0xX4BWgA1AKzLDWPuXWeCL+UFxSyotf7+HJz5IIDQrizzcOYlx0xNndQyY9yQn19QugIBs6DIbrnobBN0GDs/xHQ+o1N8/gi4GfW2vXGGPCgQRjzGfW2i0ujinimrX7MvnNu5tIPJjNmP7t+OMNg+jY/Azv/FhSDEmLnGvXdy93mqYDbnBuz6umqdQQ1wLeWnsQOOj7OscYkwh0BhTwUqdk5xfx98VJvBq3l3bhDfnvhOFcOajDmX3wdc4hWPMyxL/oNE2bdYHRj8DwiWqaSo3zyxy8MSYSOAeIq2DbdGA6QEREhD/KEakSay0LNxzksYVbOJxbwMRzI/n55X0ID6vmpyxZC/tWOmfrWz6A0iLocQlc8w/ofQUEqxUm7nD9nWWMaQq8DTxgrc0uv91aOwOYARAVFWXdrkekKvYdPs5v39/E8m3pDO7cnDkToxncpZqXPhbkwsY3nPn1Q5ugYXNnCiZqipqm4heuBrwxJhQn3OdZa99xcyyRmlBYXMrMr3bx9JLthAYH8eh1A7jj3EiCq9NETd/muz2vr2nafjBc9xQMvllNU/ErN6+iMcBsINFa+0+3xhGpKd/szODR9zezPS2XqwZ14NHrBtKheVjVHvxt03QW7P4SgkJh4A3O7Xm7xqhpKp5w8wz+POB2YKMxZp3vZ7+x1i5ycUyRajuYlcefP0pk4YaDdGnZiNkTo7i0f/uqPTjnEKx5xbk9b/b+Mk3TO6BpO3cLFzkNN6+iWQHotEVqrcLiUmav2M1/lm6npNTywJje3HVRz9PfP6bCpunFcNUT0OdKNU2l1tA7Ueql5dvS+f0Hm9mVcYwx/dvz6HUD6NrqNHdirKhpGj3VuT1vm97+KVykGhTwUq/sP5rHHz/cwuLNqUS2bsyLk6K5pN9pplIytvtuzzvf1zQdBNf+G4bcoqap1GoKeKkX8otKmPXVLp5ZtgOAB6/oy9QLulf+0XklxbDtY+eGXyeapgPGOpc5do1V01TqBAW8BDRrLR9vSuUvixJJyczj6sEdePiaAXRuUcktBnLTfCtNX4LsFGjWGUb/1rfSVE1TqVsU8BKwNu3P4rGFW1i1+wj9OoQzb2os5/Vqc/KO1kJynHO2vuX9Mk3Tv0Kfq9Q0lTpL71wJOGk5+fzjkyTeTEihVeMG/OXGwfwouuvJi5UKj8GGE03TjdCwmdMwjZoCbft4U7xIDVLAS8DILyph9ordPLdsB4UlpUy7oAf3jO5Fs/L3jsnY7oT6uvlQkPVd03TwzdCwqSe1i7hBAS91Xvl59ssHtOc3V/cnsuwHcJQUw7bFzrXru77wNU2vd1aaRoxU01QCkgJe6rSEvZk8viiR+L2Z9OsQzvypsYwqO89eUdP0kt86K03Dq7haVaSOUsBLnbQrPZcnFiexeHMqbcMb8vgPBnNLlG+e3VpIXuWcrW9+z2madr9ITVOpd/ROlzolPaeAp5ZsY8GqZMJCgvjZZX2YekF3GjcIcZqmG990FiWlqmkqooCXOuFYQTGzvtrNjOU7KSguZUJsBPdd2ps2TRtCxg6Inw1r5zlN03YD4dp/weBb1DSVek0BL7VacUkpb8Sn8K/Pt5GeU8BVgzrw4BV96dEqDLZ/4ly7vmsZBIU4K03VNBX5lgJeaqXSUstHGw/yr8+2sSvjGFHdWvL8bSMY0boY1jwPCS9BVjKEd4JLHnZWmqppKvI9CnipVay1LN2axj8+3UbiwWz6tG/KC7cN5/JmezGrfwlb3oOSQuh+IVzxF+h7tZqmIpXQ3wypNb7ZmcHfP0li7b6jdGvdmP/8sC/XmK8JWvHwd03TEXc6jdO2fb0uV6TWU8CL59buy+Qfnybx9Y7DdGgWxn8ub8bVBR8RvGQ+5GdBuwFwzT9hyI/UNBWpBgW8eCbxYDZPfrqNzxMP0bZxMDNj0xid8z7By31N0/7XO7fnjThXTVORM6CAF7/bfCCLp5ds55PNh4gIO86CfmuJPfI+QetTyjRN74DwDl6XKlKnKeDFbzbtz+KpJdv5bEsq5zfczaKuX9P/yBLMnkKIvACuPNE0DT39k4nIaSngxXUbU7J4ask2ViQmc0tYHHGtl9H+WBJkhcOISc7nmqppKlLjFPDimvXJR3lqyXZ2Ja1ncsOlPNNkOWElOdC4P1z8pK9pGu51mSIBSwEvNW71niM8tzSJ4B2fMbXB54xquB4bFILpd52z0rTbKDVNRfzAtYA3xswBrgXSrLWD3BpHagdrLcuS0nh1SQL9DrzHX0KX0LFBBqVNO0DUbzAjJqppKuJnbp7BvwQ8A7zi4hjiseKSUj7acIBlSxZxYdb7PB+8kgahxZR0uwBiphLU7xo1TUU84lrAW2uXG2Mi3Xp+8VZ+UQnvrNrOvi9f4dr8jxgbtIeihk0IGjYJYqYR3K6f1yWK1Huez8EbY6YD0wEiIiI8rkZOJyuviIXLVmBXz+ba0qW0MMfIadGb0vOfJHSomqYitYnnAW+tnQHMAIiKirIelyOV2Jeew4qP59N15zwmmPUUE8zRyCuwF/+E8Mjz1TQVqYU8D3ipvay1bNi2g12fPk90xnuMNxlkhbYmbej/0e6iH9OmWUevSxSRU1DAy0mKi0tYueJTiv43g1H5yxlqitnXfDiZFzxOy+E3qmkqUke4eZnkAuBioI0xJgV41Fo7263x5Oxl52SxbtFs2m2dy/l2F8cJY0/ED4m48j4iOutKV5G6xs2raMa59dxSs3YlbeDA588wOG0hF5pjJIdEkDj4d/S5fBp9GzXzujwROUOaoqmnioqKWL/sTUISZjOsIJ4IG8Tm5hcRfv5d9Ii+Qk1TkQCggK9nMg4dIOnjZ+m+5w2iSCODlsRHTqfXVfcytL0uUxUJJAr4esCWlpKYsIzcr15gaNZSzjNFJDYcQsbwhxk4ejxtQht4XaKIuEABH8Ayj2ax4ZM5dEh6lQGlOzhmw1jf9jo6jLmH/v1GeF2eiLhMAR9grLWsWb+Go18+z4gjH3GROca+4AjWDHyYvldMJaZZK69LFBE/UcAHiPSs46z+/A1abXmZmOK1lBpDUsuLyL7gbiKGX06EmqYi9Y4Cvg4rKinl6w1JHPlqDlGH3+Nqk8aRoFYk9b2b7lf8hIGtu3pdooh4SAFfx1hr2Xwgm/999Skdk+ZyWek3NDRF7Ak/h9RRf6BD7M200kpTEUEBX2ekZefzYcJOMle9zmXHPmRa0C7yTSPSet1MhzE/JbKjVpqKyPcp4GuxvMISPk88xFdxq+m173VuDv6SliaXo+E9OH7uX2kcNYGuYVppKiIVU8DXMoXFpSzfls6H65Ip2Popt9hP+GvweggJ4niPK+CCu2kReYFWmorIaSnga4GSUsvKXYf5YN0Bvtm0jauKPufB0CV0CUqjsFFbiH6QoKg7adqsk9elikgdooD3SGmpZW1yJh+uP8jCDQfpdGwLkxt8zp+C/kdoaCGlEaMg5q806HcdhGilqYhUnwLej4pLSlm1+wiLN6fyyeZUjmbncEPoSt5qvIzIhknY0CaYobdB9FSC2g/0ulwRqeMU8C7LLyrh6x0ZLN6UyueJh8g8XkSv0HT+2PobLraLaVCUBeF9YPTfMUN/BGHNvS5ZRAKEAt4FOflFLN+WweLNqSzbmkZuQTHNwoK4p+tebihaRNvU5ZisIOh3DURPhe4XqmkqIjVOAV9DdmccY+nWNJZuPcSq3UcoKrG0btKAHw1szPgGy+mx53VM8l5o0g4ufBBGTILmnb0uW0QCmAL+DBWVlLJ6zxGWJqaxdGsauzKOAdC7XVMmn9+d61ofZMD+Nwna/A4U50PEKBjzKKhpKiJ+ooCvhgNH81ixPYMvt6WzfFs6OQXFNAgOYmTP1kwcFcnoXs3oemAxrPo9xK2B0CYwdJwzDdNBK01FxL8U8KeQW1DMyp2HWbEjg6+2p7Mz3TlLbxfekGuGdGR0v3ac16sNTY6nwOrZ8OKrkHcE2vSBq56AobeqaSoinlHAl1FcUsrG/Vl8tT2DFdszWLMvk+JSS1hoELHdWzMuJoILerelT/umGGth5xJ4ayZs/xRMEPS72tc0vUhNUxHxXL0O+CJfoMftOkLc7sPE78kkt6AYY2Bgp2ZMu7AHF/Rqw4jIljQMCXYedPwIfPMfiJ8NmXt8TdNfwIg71TQVkVqlXgV8QXEJG1KyiNt1mLjdR0jYm8nxwhIAerVryvXDOnFuj9ac16sNrZqUa4TuX+NMw2x6y9c0PRdGPwL9r1fTVERqJVcD3hhzJfAUEAzMstb+1c3xysvILWDN3kzWJh9lzd5M1iUfpaC4FIB+HcK5eUQXYnu0JqZ7K9o0bXjyExTlw+Z3YfVM2J8AoY19TdMp0GGwP/8oIiLV5lrAG2OCgWeBy4AUYLUx5gNr7RY3xisqKSXxYPZ3gb4vk+QjeQCEBBkGdmrGhNhuxPZoRUxkK1qWP0MvK3MPxM+BNXOdpmnr3nDl32DYODVNRaTOcPMMPgbYYa3dBWCMeQ0YC9RowBcUl3D7rFWsT/nu7Lx9s4YMj2jJ7SO7MTyiJYM6NycsNPjUT1Ra6jRNV8+CbZ84TdK+V0PMNDVNRaROcjPgOwPJZb5PAWLL72SMmQ5MB4iIiKj2IA1DgmkT3oAJsd0Y3q0FwyNa0rF5GKaqgXz8CKyb58yvZ+4u0zSdBM27VLseEZHaws2Aryhh7Uk/sHYGMAMgKirqpO1V8dyEEdV/0IG1sGpWuabpb9U0FZGA4WbApwBdy3zfBTjg4ninV5QPW96DVTNhf7yvaXqrb6WpmqYiEljcDPjVQG9jTHdgP3ArMN7F8SqXuddpmq6dC8cPf9c0HXorNGrhSUkiIm5zLeCttcXGmHuAT3Auk5xjrd3s1ngnKS2FnUt9TdPF3zVNo6dCj4vVNBWRgOfqdfDW2kXAIjfHOMnxI7BuvrPS9MguaNIWLvg5RN2ppqmI1CuBs5L1wDpnQdLGt6E4D7qOhIt/AwOuh5AKFjGJiAS4uh/wBTkw90ZIWe00TYfc4kzDdBzidWUiIp6q+wHfMBxadodBP3RuI6CmqYgIEAgBD/DDmV5XICJS6wR5XYCIiLhDAS8iEqAU8CIiAUoBLyISoBTwIiIBSgEvIhKgFPAiIgFKAS8iEqCMtWf0GRuuMMakA3vP8OFtgIwaLKemqK7qq621qa7qUV3Vdya1dbPWtq1oQ60K+LNhjIm31kZ5XUd5qqv6amttqqt6VFf11XRtmqIREQlQCngRkQAVSAE/w+sCKqG6qq+21qa6qkd1VV+N1hYwc/AiIvJ9gXQGLyIiZSjgRUQCVJ0KeGPMlcaYJGPMDmPMQxVsN8aYp33bNxhjhvuprq7GmGXGmERjzGZjzP0V7HOxMSbLGLPO9+t3fqptjzFmo2/M+Aq2+/2YGWP6ljkO64wx2caYB8rt47fjZYyZY4xJM8ZsKvOzVsaYz4wx232/t6zksad8T7pQ19+NMVt9r9W7xpgWlTz2lK+7C3X93hizv8zrdXUlj/X38Xq9TE17jDHrKnmsm8erwnzwy3vMWlsnfgHBwE6gB9AAWA8MKLfP1cDHgAFGAnF+qq0jMNz3dTiwrYLaLgYWenDc9gBtTrHdk2NW7nVNxVms4cnxAi4EhgObyvzsCeAh39cPAX+rpPZTviddqOtyIMT39d8qqqsqr7sLdf0e+EUVXmu/Hq9y258EfufB8aowH/zxHqtLZ/AxwA5r7S5rbSHwGjC23D5jgVesYyXQwhjT0e3CrLUHrbVrfF/nAIlAZ7fHrSGeHLMyLgV2WmvPdAXzWbPWLgeOlPvxWOBl39cvAzdU8NCqvCdrtC5r7afW2mLftyuBLjU13tnUVUV+P14nGGMMcAuwoKbGq6pT5IPr77G6FPCdgeQy36dwcohWZR9XGWMigXOAuAo2n2uMWW+M+dgYM9BPJVngU2NMgjFmegXbvT5mt1L5XzovjtcJ7a21B8H5Cwq0q2Afr4/dZJz/fVXkdK+7G+7xTR3NqWS6wcvjdQFwyFq7vZLtfjle5fLB9fdYXQp4U8HPyl/jWZV9XGOMaQq8DTxgrc0ut3kNzjTEUOA/wHt+Kus8a+1w4Crgp8aYC8tt9+yYGWMaANcDb1aw2avjVR1eHruHgWJgXiW7nO51r2n/BXoCw4CDONMh5Xn593Mcpz57d/14nSYfKn1YBT+r8jGrSwGfAnQt830X4MAZ7OMKY0wozos3z1r7Tvnt1tpsa22u7+tFQKgxpo3bdVlrD/h+TwPexfkvX1meHTOcv0xrrLWHym/w6niVcejEVJXv97QK9vHk2BljJgLXAhOsb6K2vCq87jXKWnvIWltirS0FZlYynlfHKwT4AfB6Zfu4fbwqyQfX32N1KeBXA72NMd19Z363Ah+U2+cD4A7flSEjgawT/wVyk29+bzaQaK39ZyX7dPDthzEmBufYH3a5ribGmPATX+M06DaV282TY+ZT6VmVF8ernA+Aib6vJwLvV7BPVd6TNcoYcyXwK+B6a+3xSvapyute03WV7dvcWMl4fj9ePmOArdbalIo2un28TpEP7r/H3Ogau/UL54qPbThd5Yd9P7sLuMv3tQGe9W3fCET5qa7zcf7btAFY5/t1dbna7gE243TBVwKj/FBXD994631j16Zj1hgnsJuX+ZknxwvnH5mDQBHOGdMUoDWwBNju+72Vb99OwKJTvSddrmsHzpzsiffZ8+Xrqux1d7muub73zwacAOpYG46X7+cvnXhfldnXn8ersnxw/T2mWxWIiASoujRFIyIi1aCAFxEJUAp4EZEApYAXEQlQCngRkQClgBcRCVAKeBGRAKWAF6mEMSbad/OsMN9qx83GmEFe1yVSVVroJHIKxpg/AWFAIyDFWvu4xyWJVJkCXuQUfPf/WA3k49wuocTjkkSqTFM0IqfWCmiK80k8YR7XIlItOoMXOQVjzAc4n6LTHecGWvd4XJJIlYV4XYBIbWWMuQMottbON8YEA98YY0Zba5d6XZtIVegMXkQkQGkOXkQkQCngRUQClAJeRCRAKeBFRAKUAl5EJEAp4EVEApQCXkQkQP0/psF0qowwoBEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''미분 구현\n",
    "미분의 정의에 따라서 구현 했지만 아래의 함수는 개선점이 2가지 있음\n",
    "1. 엄청 작은 값을 사용하고 싶어서 1e-50을 사용했지만 이는 반올림 오차 문제, 즉 올바로 표현할 수 없음 -> 1e-4 정도로 바꾸어야함\n",
    "2. f의 차분과 관련된 것인데, 진정한 미분은 h를 무한히 0으로 좁히는 것인데 여기서 구현할 수 없기에 중심 차분 혹은 중앙차분으로 재구현 해야함\n",
    "    우리가 기존에 보던 미분은 엄밀한 '해석적 미분' 이고 여기서 사용하는 미분은 '수치 미분'으로 근사치를 이용해서 미분값을 구하는 방식을 의미함\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-50\n",
    "    return ((f(x+h) - f(x)))/h\n",
    "\n",
    "def numerical_diff_new(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h))/(2*h)\n",
    "\n",
    "def func1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "# y-f(a) = f'(a)(x-a)\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff_new(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(func1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000000000012662\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "편미분 구현\n",
    "'''\n",
    "\n",
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    #또는 return np.sum(x**2)\n",
    "\n",
    "#x0 = 3, x1 = 4일 떄 x0에 대한 편미분 값은?\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0+2.0\n",
    "\n",
    "print(numerical_diff_new(function_tmp1, 3.00))\n",
    "\n",
    "#x0=3, x1 = 4일 때, x1에 대한 편미분 값은?\n",
    "def function_tmp2(x1):\n",
    "    return 3*2 + x1*x1\n",
    "\n",
    "print(numerical_diff_new(function_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "위에서는 각각의 편미분 값을 구했지만 여기서는 한번에 구할 수 있는 방법을 구현해본다. \n",
    "만약 입력을 정수형으로 주면 1e-4의 값이 무시 되어서 무조건 실수형으로 인풋을 주어야 한다. \n",
    "'''\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.size):\n",
    "        tmp_val = x[i]\n",
    "        x[i] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[i] = tmp_val-h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[i] = (fxh1 - fxh2)/(2*h)\n",
    "        \n",
    "        x[i] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "numerical_gradient(func2, np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.47262693e-21 5.47262693e-21]\n",
      "[4.99999996 4.99999996]\n",
      "[-1.80220317e+12 -1.80220317e+12]\n",
      "[-1.5650224e+12 -1.5650224e+12]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num =100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad =  numerical_gradient(f,x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "\n",
    "# 아래의 결과를 통해서 학습률을 너무 크지도 너무 적지도 않게 정해야하는 이유를 알 수 있다. \n",
    "init_x = np.array([5.,5.])\n",
    "print(gradient_descent(function_2, init_x,0.1, 1000))\n",
    "init_x = np.array([5.,5.])\n",
    "print(gradient_descent(function_2, init_x,1, 1000))\n",
    "init_x = np.array([5.,5.])\n",
    "print(gradient_descent(function_2, init_x,10, 1000))\n",
    "init_x = np.array([5.,5.])\n",
    "print(gradient_descent(function_2, init_x,100, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습 알고리즘 구현하기\n",
    "- 신경망 학습의 절차\n",
    "    1. 미니배치\n",
    "        - 훈련 데이터 중 일부를 무작위로 가져와서 그 미니 배치의 손실함수 값을 줄이기\n",
    "    2. 기울기 산출\n",
    "        - 미니배치의 손실 함수 값을 줄이기 위해서 각 가중치 매개변수의 기울기를 구함\n",
    "    3. 매개변수 갱신\n",
    "        - 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "    4. 반복\n",
    "'''\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std / np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std / np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/ (1+np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        c = np.max(x)\n",
    "        exp_a = np.exp(x-c)\n",
    "        sum_exp_a = np.sumn(exp_a)\n",
    "        y = exp_a/sum_exp_a\n",
    "        return y\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = self.sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = self.softmax(a2)\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, aixs = 1)\n",
    "        t = np.argmax(t, aixs = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x, t) \n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4e3c63a28d2eadc36e3bba0725a6dea388233dd4d06cf5319c6ed40df7370ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
